# Qwen3 Fine-tuning Project

## é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªåŸºäºQwen3æ¨¡å‹çš„å¾®è°ƒé¡¹ç›®ï¼Œä½¿ç”¨LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚é¡¹ç›®åŒ…å«æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„å®Œæ•´æµç¨‹ã€‚

## åŠŸèƒ½ç‰¹æ€§

- âœ¨ åŸºäºQwen3-0.6Bæ¨¡å‹çš„LoRAå¾®è°ƒ
- ğŸ› ï¸ è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†å’Œè½¬æ¢
- ğŸ“Š æ”¯æŒä¸­è‹±æ–‡è‡ªæˆ‘è®¤çŸ¥æ•°æ®
- ğŸš€ ä½¿ç”¨MS-Swiftæ¡†æ¶è¿›è¡Œé«˜æ•ˆè®­ç»ƒ
- ğŸ’¾ æ”¯æŒæ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤

## é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ transform_data.py          # æ•°æ®è½¬æ¢è„šæœ¬
â”œâ”€â”€ terminal.sh               # è®­ç»ƒå’Œæ¨ç†å‘½ä»¤
â”œâ”€â”€ self_cognition.jsonl      # åŸå§‹æ•°æ®é›†
â”œâ”€â”€ self_cognition_futureai.jsonl  # è½¬æ¢åçš„è®­ç»ƒæ•°æ®
â”œâ”€â”€ requirements.txt          # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ .gitignore               # Gitå¿½ç•¥æ–‡ä»¶
â””â”€â”€ README.md               # é¡¹ç›®æ–‡æ¡£
```

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

é¦–å…ˆå‡†å¤‡ä½ çš„è‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†æ ¼å¼ï¼Œç„¶åè¿è¡Œæ•°æ®è½¬æ¢ï¼š

```bash
python transform_data.py \
    --name_zh å°æ–° \
    --author_zh FutureAIå®éªŒå®¤ \
    --name_en Xiao-xin \
    --author_en FutureAILab
```

### 3. ä¸‹è½½åŸºç¡€æ¨¡å‹

```bash
modelscope download --model Qwen/Qwen3-0.6B --local_dir ./models/Qwen/Qwen3-0.6B
```

### 4. å¼€å§‹è®­ç»ƒ

```bash
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model ./models/Qwen/Qwen3-0.6B \
    --train_type lora \
    --dataset './self_cognition_futureai.jsonl' \
    --torch_dtype bfloat16 \
    --num_train_epochs 5 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_step 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir ./outputs \
    --system 'You are a helpful assistant.' \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4
```

### 5. æ¨ç†æµ‹è¯•

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters ./outputs/checkpoint-XXX \
    --stream true \
    --temperature 0 \
    --max_new_tokens 2048
```

## æ ¸å¿ƒä¾èµ–

- **PyTorch** - æ·±åº¦å­¦ä¹ æ¡†æ¶
- **Transformers** - Hugging Faceæ¨¡å‹åº“
- **ModelScope** - æ¨¡å‹ä¸‹è½½å’Œç®¡ç†
- **MS-Swift** - é«˜æ•ˆå¾®è°ƒæ¡†æ¶
- **PEFT** - å‚æ•°é«˜æ•ˆå¾®è°ƒ
- **LoRA** - ä½ç§©é€‚åº”æŠ€æœ¯

## æ•°æ®æ ¼å¼

é¡¹ç›®æ”¯æŒJSONLæ ¼å¼çš„æ•°æ®é›†ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```json
{"conversations": [{"from": "human", "value": "ä½ æ˜¯è°ï¼Ÿ"}, {"from": "gpt", "value": "æˆ‘æ˜¯{{NAME}}ï¼Œç”±{{AUTHOR}}å¼€å‘çš„AIåŠ©æ‰‹ã€‚"}], "tag": "zh"}
```

## ä¸»è¦å‚æ•°è¯´æ˜

- **lora_rank**: LoRAçŸ©é˜µçš„ç§©ï¼Œå½±å“å‚æ•°é‡å’Œæ€§èƒ½
- **lora_alpha**: LoRAçš„ç¼©æ”¾å‚æ•°ï¼Œæ§åˆ¶æ›´æ–°å¼ºåº¦
- **learning_rate**: å­¦ä¹ ç‡ï¼Œå»ºè®®1e-4
- **warmup_ratio**: é¢„çƒ­æ¯”ä¾‹ï¼Œå»ºè®®0.05
- **max_length**: è¾“å…¥åºåˆ—æœ€å¤§é•¿åº¦

## æ³¨æ„äº‹é¡¹

1. ç¡®ä¿GPUå†…å­˜å……è¶³ï¼ˆå»ºè®®8GB+ï¼‰
2. è®­ç»ƒå‰æ£€æŸ¥æ•°æ®é›†æ ¼å¼æ­£ç¡®
3. æ ¹æ®ç¡¬ä»¶è°ƒæ•´batch_sizeå’Œgradient_accumulation_steps
4. å»ºè®®è®¾ç½®save_total_limitæ¥é™åˆ¶ä¿å­˜çš„æ£€æŸ¥ç‚¹æ•°é‡

## è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·é€šè¿‡GitHub Issuesè”ç³»æˆ‘ä»¬ã€‚